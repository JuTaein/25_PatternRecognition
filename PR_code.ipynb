{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please add the original train/test datasets as values for the keys 'origin_tr' and 'origin_te' within the 'paths' variable.  \n",
        "  \n",
        "The prediction results for the test dataset will be saved in the directory as prediction.csv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT_Z7on_vFBn",
        "outputId": "9f6b8da7-9f12-4dfe-a57e-e8fb4ed352fe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "paths = {\n",
        "    'origin_tr': 'train.csv',\n",
        "    'origin_te': 'test.csv'\n",
        "}\n",
        "\n",
        "train = pd.read_csv(paths['origin_tr'])\n",
        "test  = pd.read_csv(paths['origin_te'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6x9rzHcw7eT"
      },
      "source": [
        "top15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PxPyFE_0wZQR"
      },
      "outputs": [],
      "source": [
        "train_id = train[\"id\"]\n",
        "test_id = test[\"id\"]\n",
        "\n",
        "y = train[\"y\"]\n",
        "X = train.drop(columns=[\"id\", \"y\", \"shares\"])\n",
        "test_15 = test.drop(columns=[\"id\"])  # id\n",
        "\n",
        "#train의 median\n",
        "train_medians = X.median(numeric_only=True)\n",
        "X     = X.fillna(train_medians)\n",
        "test_15 = test_15.fillna(train_medians)\n",
        "\n",
        "X = pd.get_dummies(X) # 범주형 변수 원핫 인코딩\n",
        "test_15 = pd.get_dummies(test_15)\n",
        "\n",
        "test_15 = test_15.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "# 여기까지가 train/test_basic 데이터\n",
        "\n",
        "# Top15 features 리스트 (XGBoost gain 기준)\n",
        "top15_features = [\n",
        "    'kw_avg_avg', 'data_channel_Entertainment', 'kw_max_avg', 'data_channel_Tech',\n",
        "    'self_reference_min_shares', 'weekday_Saturday', 'self_reference_avg_sharess',\n",
        "    'weekday_Sunday', 'data_channel_Social Media', 'kw_avg_max',\n",
        "    'LDA_00', 'LDA_02', 'kw_min_avg', 'kw_avg_min', 'num_hrefs'\n",
        "]\n",
        "\n",
        "# 파생변수 정의\n",
        "def add_derived_features(df):\n",
        "    df = df.copy()\n",
        "    df[\"keyword_strength_ratio\"] = df[\"kw_max_avg\"] / df[\"kw_avg_avg\"].replace(0, 1e-9)\n",
        "    df[\"img_token_ratio\"] = df[\"num_imgs\"] / df[\"n_unique_tokens\"].replace(0, 1e-9)\n",
        "    df[\"subjectivity_sentiment_mix\"] = df[\"global_subjectivity\"] * df[\"global_rate_positive_words\"]\n",
        "    return df\n",
        "\n",
        "X_full = add_derived_features(X)\n",
        "test_full = add_derived_features(test_15)\n",
        "\n",
        "derived_features = [\"keyword_strength_ratio\", \"img_token_ratio\", \"subjectivity_sentiment_mix\"]\n",
        "selected_features = top15_features + derived_features\n",
        "\n",
        "train_top15 = X_full[selected_features].copy()\n",
        "train_top15.insert(0, \"id\", train_id)\n",
        "train_top15[\"y\"] = y\n",
        "test_top15 = test_full[selected_features].copy()\n",
        "test_top15.insert(0, \"id\", test_id)\n",
        "\n",
        "#train_top15 / test_top15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6cHeb_9xY--"
      },
      "source": [
        "vif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2WBRJzCwxZ-0"
      },
      "outputs": [],
      "source": [
        "df = train\n",
        "\n",
        "df_1 = df.drop(columns=[\n",
        "    'LDA_00','LDA_01','LDA_02','LDA_03',\n",
        "    'rate_negative_words','n_non_stop_words',\n",
        "    'self_reference_avg_sharess','n_unique_tokens',\n",
        "    'kw_max_min','kw_avg_avg','y','shares','id'\n",
        "])\n",
        "\n",
        "# 결측치 처리\n",
        "df_1['data_channel'] = df_1['data_channel'].fillna('Missing')\n",
        "df_1['weekday']      = df_1['weekday'].fillna('Missing')\n",
        "\n",
        "# one-hot encoding (data_channel)\n",
        "df_one_hot = pd.get_dummies(\n",
        "    df_1,\n",
        "    columns=['data_channel'],\n",
        "    prefix='channel',\n",
        "    drop_first=True\n",
        ")\n",
        "channel_cols = [c for c in df_one_hot.columns if c.startswith('channel_')]\n",
        "\n",
        "\n",
        "# LabelEncoder (weekday)\n",
        "weekday_order = [\n",
        "    'Monday','Tuesday','Wednesday',\n",
        "    'Thursday','Friday','Saturday','Sunday','Missing'\n",
        "]\n",
        "le = LabelEncoder().fit(weekday_order)\n",
        "df_one_hot['weekday_encoded'] = le.transform(df_one_hot['weekday']) + 1\n",
        "\n",
        "# weekday 원본 컬럼 제거, float 타입 변환\n",
        "df_2 = df_one_hot.drop(columns=['weekday']).astype(float)\n",
        "\n",
        "\n",
        "log1p_clip_cols = [\n",
        "    'n_tokens_content', 'num_hrefs', 'num_imgs', 'num_videos',\n",
        "    'kw_min_min', 'kw_min_avg', 'kw_min_max',\n",
        "    'kw_avg_min', 'kw_avg_avg', 'kw_avg_max',\n",
        "    'kw_max_min', 'kw_max_avg', 'kw_max_max',\n",
        "    'self_reference_min_shares', 'self_reference_max_shares'\n",
        "]\n",
        "binary_split_cols = [\n",
        "    'n_non_stop_unique_tokens',\n",
        "    'kw_min_min', 'kw_avg_min', 'kw_min_avg', 'num_videos'\n",
        "]\n",
        "log1p_clip_standard_cols = [\n",
        "    'title_subjectivity', 'title_sentiment_polarity', 'abs_title_sentiment_polarity'\n",
        "]\n",
        "standard_only_cols = [\n",
        "    'n_tokens_title', 'average_token_length', 'num_keywords', 'LDA_04',\n",
        "    'global_subjectivity', 'global_sentiment_polarity', 'rate_positive_words',\n",
        "    'avg_positive_polarity', 'min_positive_polarity', 'max_positive_polarity',\n",
        "    'avg_negative_polarity', 'min_negative_polarity', 'max_negative_polarity',\n",
        "    'abs_title_subjectivity'\n",
        "]\n",
        "\n",
        "# 2) df_proc 복사\n",
        "df_proc = df_2.copy()\n",
        "\n",
        "# 3) log1p 변환 + IQR 클리핑 (bounds 계산 및 저장)\n",
        "iqr_bounds = {}\n",
        "for col in log1p_clip_cols + log1p_clip_standard_cols:\n",
        "    if col in df_proc.columns:\n",
        "        # (a) shift 후 log1p\n",
        "        min_val = df_proc[col].min()\n",
        "        if min_val <= -1:\n",
        "            df_proc[col] += abs(min_val) + 1.001\n",
        "        df_proc[col] = np.log1p(df_proc[col])\n",
        "        # (b) IQR bounds 계산\n",
        "        q1 = df_proc[col].quantile(0.25)\n",
        "        q3 = df_proc[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
        "        iqr_bounds[col] = (lower, upper)\n",
        "        # (c) clip\n",
        "        df_proc[col] = df_proc[col].clip(lower, upper)\n",
        "\n",
        "# 4) binary split\n",
        "for col in binary_split_cols:\n",
        "    if col in df_proc.columns:\n",
        "        df_proc[col + '_binary'] = (df_proc[col] > 0).astype(int)\n",
        "\n",
        "# 5) standard scaling (fit & transform)\n",
        "scaler = StandardScaler()\n",
        "scale_targets = [\n",
        "    c for c in df_proc.columns\n",
        "    if c not in ['weekday_encoded'] + channel_cols\n",
        "]\n",
        "df_proc[scale_targets] = scaler.fit_transform(df_proc[scale_targets])\n",
        "\n",
        "# 6) id, y 컬럼 복원\n",
        "id_col = df['id'].reset_index(drop=True)\n",
        "y_col  = df['y'].reset_index(drop=True)\n",
        "df_proc.insert(0, 'id', id_col)\n",
        "df_proc['y'] = y_col\n",
        "\n",
        "train_vif = df_proc\n",
        "\n",
        "\n",
        "df_raw = test\n",
        "id_col = df_raw['id']\n",
        "\n",
        "# 2) train과 동일한 컬럼 제거 & 결측치 처리\n",
        "drop_cols = [\n",
        "    'LDA_00','LDA_01','LDA_02','LDA_03',\n",
        "    'rate_negative_words','n_non_stop_words',\n",
        "    'self_reference_avg_sharess','n_unique_tokens',\n",
        "    'kw_max_min','kw_avg_avg','y','shares','id'\n",
        "]\n",
        "df1 = df_raw.drop(columns=drop_cols, errors='ignore')\n",
        "df1['data_channel'] = df1['data_channel'].fillna('Missing')\n",
        "df1['weekday']      = df1['weekday'].fillna('Missing')\n",
        "\n",
        "# 3) one-hot encoding + 없는 channel_cols 은 0으로 채우기\n",
        "df1 = pd.get_dummies(df1,\n",
        "                     columns=['data_channel'],\n",
        "                     prefix='channel',\n",
        "                     drop_first=True)\n",
        "for c in channel_cols:   # train 에서 정의해 둔 리스트\n",
        "    if c not in df1.columns:\n",
        "        df1[c] = 0\n",
        "\n",
        "# 4) weekday LabelEncoder 적용\n",
        "df1['weekday_encoded'] = le.transform(df1['weekday']) + 1\n",
        "df1.drop(columns=['weekday'], inplace=True)\n",
        "\n",
        "# 5) float 변환\n",
        "df2 = df1.astype(float)\n",
        "\n",
        "# 6) log1p 변환 + IQR 클리핑\n",
        "df_test = df2.copy()\n",
        "for col in log1p_clip_cols + log1p_clip_standard_cols:\n",
        "    if col in df_test.columns:\n",
        "        # (a) 음수 shift 보정\n",
        "        m = df_test[col].min()\n",
        "        if m <= -1:\n",
        "            df_test[col] += abs(m) + 1.001\n",
        "        # (b) 로그 변환\n",
        "        df_test[col] = np.log1p(df_test[col])\n",
        "        # (c) train에서 계산해 둔 lower/upper 로 클리핑\n",
        "        low, high = iqr_bounds[col]\n",
        "        df_test[col] = df_test[col].clip(low, high)\n",
        "\n",
        "# 7) binary split\n",
        "for col in binary_split_cols:\n",
        "    if col in df_test.columns:\n",
        "        df_test[col + '_binary'] = (df_test[col] > 0).astype(int)\n",
        "\n",
        "# 8) standard scaling\n",
        "scale_targets = [c for c in df_test.columns\n",
        "                 if c not in ['weekday_encoded'] + channel_cols]\n",
        "df_test[scale_targets] = scaler.transform(df_test[scale_targets])\n",
        "\n",
        "# 9) id 컬럼 맨 앞으로 복원\n",
        "df_test.insert(0, 'id', id_col)\n",
        "\n",
        "test_vif = df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaV_LDtTxKB0"
      },
      "source": [
        "ori+top15+vif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Exa69rt8xJpe"
      },
      "outputs": [],
      "source": [
        "df_top15 = train_top15\n",
        "df_vif = train_vif\n",
        "df_origin = train\n",
        "\n",
        "# Top15에서 VIF에는 없는 column\n",
        "cols_to_add = [\n",
        "    'kw_avg_avg',\n",
        "    'self_reference_avg_sharess',\n",
        "    'LDA_00',\n",
        "    'LDA_02',\n",
        "]\n",
        "\n",
        "df_top15_sel = df_top15[['id'] + cols_to_add]\n",
        "\n",
        "# VIF 세트에 병합\n",
        "df = df_vif.merge(df_top15_sel, on='id', how='left')\n",
        "\n",
        "# 파생변수 생성\n",
        "df_origin['token_density']     = df_origin['n_tokens_content'] / (df_origin['num_keywords'] + 1)\n",
        "\n",
        "lda_cols = ['LDA_00','LDA_01','LDA_02','LDA_03','LDA_04']\n",
        "lda_probs = df_origin[lda_cols].clip(lower=1e-6)\n",
        "df_origin['lda_entropy']       = -np.sum(lda_probs * np.log2(lda_probs), axis=1)\n",
        "\n",
        "df_origin['sentiment_ratio']   = df_origin['global_rate_positive_words'] / (df_origin['global_rate_negative_words'] + 1e-5)\n",
        "df_origin['emotion_contrast']  = np.abs(df_origin['avg_positive_polarity'] - df_origin['avg_negative_polarity'])\n",
        "\n",
        "df_origin['kw_maxmax_per_keyword']         = df_origin['kw_max_max'] / (df_origin['num_keywords'] + 1e-5)\n",
        "df_origin['unique_token_ratio']            = df_origin['n_non_stop_unique_tokens'] / (df_origin['n_tokens_content'] + 1e-5)\n",
        "df_origin['ref_share_ratio']               = df_origin['self_reference_max_shares'] / (df_origin['self_reference_avg_sharess'] + 1e-5)\n",
        "df_origin['share_per_token']               = df_origin['self_reference_avg_sharess'] / (df_origin['n_tokens_content'] + 1e-5)\n",
        "df_origin['kw_variety_ratio']              = df_origin['kw_max_max'] / (df_origin['kw_min_min'] + 1e-5)\n",
        "df_origin['positive_minus_negative']       = df_origin['avg_positive_polarity'] - df_origin['avg_negative_polarity']\n",
        "df_origin['sentiment_weighted_subjectivity']= df_origin['global_sentiment_polarity'] * df_origin['global_subjectivity']\n",
        "\n",
        "derived_cols = [\n",
        "    'token_density',\n",
        "    'lda_entropy',\n",
        "    'sentiment_ratio',\n",
        "    'emotion_contrast',\n",
        "    'kw_maxmax_per_keyword',\n",
        "    'unique_token_ratio',\n",
        "    'ref_share_ratio',\n",
        "    'share_per_token',\n",
        "    'kw_variety_ratio',\n",
        "    'positive_minus_negative',\n",
        "    'sentiment_weighted_subjectivity'\n",
        "]\n",
        "\n",
        "df = df.merge(\n",
        "    df_origin[['id'] + derived_cols],\n",
        "    on='id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "df.drop(columns=['kw_max_max', 'positive_minus_negative', 'global_sentiment_polarity', 'emotion_contrast'], inplace=True)\n",
        "\n",
        "df['kw_avg_avg_log'] = np.log1p(df['kw_avg_avg'])\n",
        "q = df['kw_avg_avg_log'].quantile(0.99)\n",
        "df['kw_avg_avg_log_clipped'] = df['kw_avg_avg_log'].clip(upper=q)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df['kw_avg_avg_scaled'] = scaler.fit_transform(df[['kw_avg_avg_log_clipped']])\n",
        "\n",
        "df['self_reference_avg_sharess_log'] = np.log1p(df['self_reference_avg_sharess'])\n",
        "q99 = df['self_reference_avg_sharess_log'].quantile(0.99)\n",
        "df['self_reference_avg_sharess_log_clipped'] = df['self_reference_avg_sharess_log'].clip(upper=q99)\n",
        "df['self_reference_avg_sharess_scaled'] = scaler.fit_transform( df[['self_reference_avg_sharess_log_clipped']])\n",
        "\n",
        "df['token_density_log'] = np.log1p(df['token_density'])\n",
        "q99 = df['token_density_log'].quantile(0.99)\n",
        "df['token_density_log_clip'] = df['token_density_log'].clip(upper=q99)\n",
        "df['token_density_scaled'] = scaler.fit_transform( df[['token_density_log_clip']])\n",
        "\n",
        "df['kw_maxmax_per_keyword_log'] = np.log1p(df['kw_maxmax_per_keyword'])\n",
        "q99 = df['kw_maxmax_per_keyword_log'].quantile(0.99)\n",
        "df['kw_maxmax_per_keyword_log_clipped'] = (\n",
        "    df['kw_maxmax_per_keyword_log'].clip(upper=q99)\n",
        ")\n",
        "df['kw_maxmax_per_keyword_scaled'] = scaler.fit_transform(\n",
        "    df[['kw_maxmax_per_keyword_log_clipped']]\n",
        ")\n",
        "\n",
        "\n",
        "df['unique_token_ratio_log'] = np.log1p(df['unique_token_ratio'])\n",
        "q99 = df['unique_token_ratio_log'].quantile(0.99)\n",
        "df['unique_token_ratio_log_clip'] = df['unique_token_ratio_log'].clip(upper=q99)\n",
        "df['unique_token_ratio_scaled'] = scaler.fit_transform(\n",
        "    df[['unique_token_ratio_log_clip']]\n",
        ")\n",
        "\n",
        "df['ref_share_ratio_log'] = np.log1p(df['ref_share_ratio'])\n",
        "q99 = df['ref_share_ratio_log'].quantile(0.99)\n",
        "df['ref_share_ratio_log_clip'] = df['ref_share_ratio_log'].clip(upper=q99)\n",
        "df['ref_share_ratio_scaled'] = scaler.fit_transform(\n",
        "    df[['ref_share_ratio_log_clip']]\n",
        ")\n",
        "\n",
        "# --- train 코드에서 로그 변환·클리핑·스케일링 하는 부분 바로 뒤에 ---\n",
        "# 1) kw_avg_avg\n",
        "q_kw   = df['kw_avg_avg_log'].quantile(0.99)\n",
        "scaler_kw = StandardScaler().fit(df[['kw_avg_avg_log_clipped']])\n",
        "\n",
        "# 2) self_reference_avg_sharess\n",
        "q_sr   = df['self_reference_avg_sharess_log'].quantile(0.99)\n",
        "scaler_sr = StandardScaler().fit(df[['self_reference_avg_sharess_log_clipped']])\n",
        "\n",
        "# 3) token_density\n",
        "q_td   = df['token_density_log'].quantile(0.99)\n",
        "scaler_td = StandardScaler().fit(df[['token_density_log_clip']])\n",
        "\n",
        "# 4) kw_maxmax_per_keyword\n",
        "q_km   = df['kw_maxmax_per_keyword_log'].quantile(0.99)\n",
        "scaler_km = StandardScaler().fit(df[['kw_maxmax_per_keyword_log_clipped']])\n",
        "\n",
        "# 5) unique_token_ratio\n",
        "q_ut   = df['unique_token_ratio_log'].quantile(0.99)\n",
        "scaler_ut = StandardScaler().fit(df[['unique_token_ratio_log_clip']])\n",
        "\n",
        "# 6) ref_share_ratio\n",
        "q_rf   = df['ref_share_ratio_log'].quantile(0.99)\n",
        "scaler_rf = StandardScaler().fit(df[['ref_share_ratio_log_clip']])\n",
        "\n",
        "train_final = df\n",
        "\n",
        "df_t_origin = test\n",
        "df_t_top15  = test_top15\n",
        "df_t_vif    = test_vif\n",
        "\n",
        "\n",
        "cols_to_add = [\n",
        "    'kw_avg_avg',\n",
        "    'self_reference_avg_sharess',\n",
        "    'LDA_00',\n",
        "    'LDA_02',\n",
        "]\n",
        "df_t_top15_sel = df_t_top15[['id'] + cols_to_add]\n",
        "df_test = df_t_vif.merge(df_t_top15_sel, on='id', how='left')\n",
        "\n",
        "# 3. 파생변수 생성 (train과 동일)\n",
        "df_t_origin['token_density'] = df_t_origin['n_tokens_content'] / (df_t_origin['num_keywords'] + 1)\n",
        "lda_cols = ['LDA_00','LDA_01','LDA_02','LDA_03','LDA_04']\n",
        "lda_probs = df_t_origin[lda_cols].clip(lower=1e-6)\n",
        "df_t_origin['lda_entropy'] = -np.sum(lda_probs * np.log2(lda_probs), axis=1)\n",
        "df_t_origin['sentiment_ratio']    = df_t_origin['global_rate_positive_words'] / (df_t_origin['global_rate_negative_words'] + 1e-5)\n",
        "df_t_origin['emotion_contrast']   = np.abs(df_t_origin['avg_positive_polarity'] - df_t_origin['avg_negative_polarity'])\n",
        "df_t_origin['kw_maxmax_per_keyword']        = df_t_origin['kw_max_max'] / (df_t_origin['num_keywords'] + 1e-5)\n",
        "df_t_origin['unique_token_ratio']           = df_t_origin['n_non_stop_unique_tokens'] / (df_t_origin['n_tokens_content'] + 1e-5)\n",
        "df_t_origin['ref_share_ratio']              = df_t_origin['self_reference_max_shares'] / (df_t_origin['self_reference_avg_sharess'] + 1e-5)\n",
        "df_t_origin['share_per_token']              = df_t_origin['self_reference_avg_sharess'] / (df_t_origin['n_tokens_content'] + 1e-5)\n",
        "df_t_origin['kw_variety_ratio']             = df_t_origin['kw_max_max'] / (df_t_origin['kw_min_min'] + 1e-5)\n",
        "df_t_origin['positive_minus_negative']      = df_t_origin['avg_positive_polarity'] - df_t_origin['avg_negative_polarity']\n",
        "df_t_origin['sentiment_weighted_subjectivity'] = df_t_origin['global_sentiment_polarity'] * df_t_origin['global_subjectivity']\n",
        "\n",
        "derived_cols = [\n",
        "    'token_density',\n",
        "    'lda_entropy',\n",
        "    'sentiment_ratio',\n",
        "    'emotion_contrast',\n",
        "    'kw_maxmax_per_keyword',\n",
        "    'unique_token_ratio',\n",
        "    'ref_share_ratio',\n",
        "    'share_per_token',\n",
        "    'kw_variety_ratio',\n",
        "    'positive_minus_negative',\n",
        "    'sentiment_weighted_subjectivity'\n",
        "]\n",
        "\n",
        "df_test = df_test.merge(\n",
        "    df_t_origin[['id'] + derived_cols],\n",
        "    on='id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 4. 결측치·무한대 처리\n",
        "df_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df_test.fillna(df.mean(), inplace=True)  # train 데이터 평균으로 채우기\n",
        "\n",
        "df_test.drop(columns=[\n",
        "    'kw_max_max',\n",
        "    'positive_minus_negative',\n",
        "    'global_sentiment_polarity',\n",
        "    'emotion_contrast'\n",
        "], inplace=True)\n",
        "\n",
        "# 5. 로그 변환 → 클리핑 → 스케일링 (train과 똑같이)\n",
        "# (train 코드에서 이미 계산된 q_kw, q_sr, q_td, q_km, q_ut, q_rf 변수를 그대로 사용)\n",
        "# 5-1) kw_avg_avg\n",
        "df_test['kw_avg_avg_log'] = np.log1p(df_test['kw_avg_avg'])\n",
        "df_test['kw_avg_avg_log_clipped'] = df_test['kw_avg_avg_log'].clip(upper=q_kw)\n",
        "df_test['kw_avg_avg_scaled'] = scaler_kw.transform(df_test[['kw_avg_avg_log_clipped']])\n",
        "\n",
        "# 5-2) self_reference_avg_sharess\n",
        "df_test['self_reference_avg_sharess_log'] = np.log1p(df_test['self_reference_avg_sharess'])\n",
        "df_test['self_reference_avg_sharess_log_clipped'] = df_test['self_reference_avg_sharess_log'].clip(upper=q_sr)\n",
        "df_test['self_reference_avg_sharess_scaled'] = scaler_sr.transform(df_test[['self_reference_avg_sharess_log_clipped']])\n",
        "\n",
        "# 5-3) token_density\n",
        "df_test['token_density_log'] = np.log1p(df_test['token_density'])\n",
        "df_test['token_density_log_clip'] = df_test['token_density_log'].clip(upper=q_td)\n",
        "df_test['token_density_scaled'] = scaler_td.transform(df_test[['token_density_log_clip']])\n",
        "\n",
        "# 5-4) kw_maxmax_per_keyword\n",
        "df_test['kw_maxmax_per_keyword_log'] = np.log1p(df_test['kw_maxmax_per_keyword'])\n",
        "df_test['kw_maxmax_per_keyword_log_clipped'] = df_test['kw_maxmax_per_keyword_log'].clip(upper=q_km)\n",
        "df_test['kw_maxmax_per_keyword_scaled'] = scaler_km.transform(df_test[['kw_maxmax_per_keyword_log_clipped']])\n",
        "\n",
        "# 5-5) unique_token_ratio\n",
        "df_test['unique_token_ratio_log'] = np.log1p(df_test['unique_token_ratio'])\n",
        "df_test['unique_token_ratio_log_clip'] = df_test['unique_token_ratio_log'].clip(upper=q_ut)\n",
        "df_test['unique_token_ratio_scaled'] = scaler_ut.transform(df_test[['unique_token_ratio_log_clip']])\n",
        "\n",
        "# 5-6) ref_share_ratio\n",
        "df_test['ref_share_ratio_log'] = np.log1p(df_test['ref_share_ratio'])\n",
        "df_test['ref_share_ratio_log_clip'] = df_test['ref_share_ratio_log'].clip(upper=q_rf)\n",
        "df_test['ref_share_ratio_scaled'] = scaler_rf.transform(df_test[['ref_share_ratio_log_clip']])\n",
        "\n",
        "test_final = df_test\n",
        "test_final[\"id\"] = test[\"id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defining the final L2 regularized Stacking model...\n",
            "Training the final L2 regularized Stacking model on train_final (X shape: (22200, 75), y shape: (22200,))...\n",
            "Training complete.\n",
            "Predicting on test_final (X shape: (9515, 75))...\n",
            "Prediction complete.\n",
            "Results saved to prediction.csv\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Optimal hyperparameters from previous GridSearch (for the L2 regularized model) ---\n",
        "tuned_stacking_params = {\n",
        "    'rf__n_estimators': 100,\n",
        "    'rf__max_depth': 10,\n",
        "    'xgb__n_estimators': 100,\n",
        "    'xgb__learning_rate': 0.05,\n",
        "    'lgbm__n_estimators': 100,\n",
        "    'lgbm__num_leaves': 31,\n",
        "    'catb__iterations': 100,\n",
        "    'catb__depth': 4,\n",
        "    'final_estimator__C': 0.1,\n",
        "    'passthrough': False  # This was the setting for fin1\n",
        "}\n",
        "\n",
        "# --- L2 regularization settings for the chosen final model ---\n",
        "L2_XGB_LAMBDA = 1.0\n",
        "L2_LGBM_LAMBDA = 0.1\n",
        "L2_CATB_LEAF_REG = 3.0\n",
        "\n",
        "# --- Assume train_final and test_final DataFrames are already loaded and preprocessed ---\n",
        "# Example placeholders (replace with your actual data loading and preprocessing):\n",
        "# Ensure 'train_final' has the target column 'y' and features.\n",
        "# Ensure 'test_final' has an 'id' column and features.\n",
        "\n",
        "# --- Placeholder: Create dummy DataFrames for demonstration ---\n",
        "# Remove or replace this section with your actual data loading\n",
        "train_final = train_final.drop(columns=[\"id\"])\n",
        "n_features = len(train_final.columns)-1\n",
        "# --- End of Placeholder ---\n",
        "\n",
        "\n",
        "# --- Prepare X_train, y_train from train_final DataFrame ---\n",
        "if 'y' not in train_final.columns:\n",
        "    raise ValueError(\"Target column 'y' not found in train_final DataFrame.\")\n",
        "y_train = train_final['y']\n",
        "X_train = train_final.drop(columns=['y'])\n",
        "\n",
        "# --- Prepare X_test and test_ids from test_final DataFrame ---\n",
        "if 'id' not in test_final.columns:\n",
        "    raise ValueError(\"'id' column not found in test_final DataFrame.\")\n",
        "test_ids = test_final['id']\n",
        "# Assuming all other columns in test_final are features and match X_train's features\n",
        "# If 'id' is not the only non-feature column, drop others as well.\n",
        "X_test = test_final.drop(columns=['id'])\n",
        "\n",
        "\n",
        "# --- Ensure feature consistency (columns and order) ---\n",
        "# It's crucial that X_train and X_test have the same features in the same order.\n",
        "# If your preprocessing steps guarantee this, you might not need explicit reordering.\n",
        "# However, for safety, you can align columns:\n",
        "common_features = X_train.columns.intersection(X_test.columns)\n",
        "if len(common_features) != X_train.shape[1] or len(common_features) != X_test.shape[1]:\n",
        "    print(\"Warning: Feature mismatch between train and test. Using common features only.\")\n",
        "    # This part might need adjustment based on how you handle feature differences\n",
        "    # For now, let's assume they should match from your preprocessing\n",
        "    if X_train.shape[1] != X_test.shape[1]:\n",
        "        raise ValueError(f\"Feature count mismatch: X_train has {X_train.shape[1]} features, X_test has {X_test.shape[1]} features after dropping 'id'. Ensure they match.\")\n",
        "\n",
        "\n",
        "X_train = X_train[common_features]\n",
        "X_test = X_test[common_features]\n",
        "\n",
        "\n",
        "# --- Define the final L2 regularized Stacking model ---\n",
        "print(\"Defining the final L2 regularized Stacking model...\")\n",
        "rf_final_l2 = RandomForestClassifier(\n",
        "    n_estimators=tuned_stacking_params['rf__n_estimators'],\n",
        "    max_depth=tuned_stacking_params['rf__max_depth'],\n",
        "    random_state=42,\n",
        "    class_weight='balanced' # Retained from original script context\n",
        ")\n",
        "xgb_final_l2 = XGBClassifier(\n",
        "    n_estimators=tuned_stacking_params['xgb__n_estimators'],\n",
        "    learning_rate=tuned_stacking_params['xgb__learning_rate'],\n",
        "    random_state=42,\n",
        "    use_label_encoder=False, # Retained\n",
        "    eval_metric='logloss',   # Retained\n",
        "    reg_lambda=L2_XGB_LAMBDA # L2 regularization\n",
        ")\n",
        "lgbm_final_l2 = LGBMClassifier(\n",
        "    n_estimators=tuned_stacking_params['lgbm__n_estimators'],\n",
        "    num_leaves=tuned_stacking_params['lgbm__num_leaves'],\n",
        "    random_state=42,\n",
        "    class_weight='balanced', # Retained\n",
        "    verbosity=-1,            # Retained\n",
        "    reg_lambda=L2_LGBM_LAMBDA # L2 regularization\n",
        ")\n",
        "catb_final_l2 = CatBoostClassifier(\n",
        "    iterations=tuned_stacking_params['catb__iterations'],\n",
        "    depth=tuned_stacking_params['catb__depth'],\n",
        "    random_seed=42,          # Retained\n",
        "    verbose=0,               # Retained\n",
        "    l2_leaf_reg=L2_CATB_LEAF_REG # L2 regularization\n",
        ")\n",
        "lr_final_l2 = LogisticRegression(\n",
        "    C=tuned_stacking_params['final_estimator__C'],\n",
        "    solver='liblinear',      # Retained\n",
        "    random_state=42,\n",
        "    class_weight='balanced', # Retained\n",
        "    max_iter=1000,           # Retained\n",
        "    penalty='l2' # Ensure final estimator also uses L2 if desired (already implied by C for LogisticRegression)\n",
        ")\n",
        "\n",
        "stacking_cv_splitter = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "final_stacking_model_l2 = StackingClassifier(\n",
        "    estimators=[('rf', rf_final_l2), ('xgb', xgb_final_l2), ('lgbm', lgbm_final_l2), ('catb', catb_final_l2)],\n",
        "    final_estimator=lr_final_l2,\n",
        "    passthrough=tuned_stacking_params['passthrough'],\n",
        "    cv=stacking_cv_splitter\n",
        ")\n",
        "\n",
        "# --- Train the final model on the entire train_final dataset ---\n",
        "print(f\"Training the final L2 regularized Stacking model on train_final (X shape: {X_train.shape}, y shape: {y_train.shape})...\")\n",
        "final_stacking_model_l2.fit(X_train, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# --- Make predictions on the test_final dataset ---\n",
        "print(f\"Predicting on test_final (X shape: {X_test.shape})...\")\n",
        "test_pred_proba = final_stacking_model_l2.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "custom_threshold = 0.47\n",
        "test_pred_y = (test_pred_proba >= custom_threshold).astype(int) #astype(int) converts boolean to 0 or 1\n",
        "print(\"Prediction complete.\")\n",
        "\n",
        "# --- Create a DataFrame for the results ---\n",
        "results_df = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'y_predict': test_pred_y,\n",
        "    'y_prob': test_pred_proba\n",
        "})\n",
        "\n",
        "# --- Save the results to a CSV file ---\n",
        "output_filename = 'prediction.csv'\n",
        "results_df.to_csv(output_filename, index=False)\n",
        "print(f\"Results saved to {output_filename}\")\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
